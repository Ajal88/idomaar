{
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "metadata": {},
     "outputs": [],
     "input": [
      "import json\n",
      "from pyspark.mllib.recommendation import ALS"
     ],
     "language": "python",
     "prompt_number": 29
    },
    {
     "cell_type": "heading",
     "metadata": {},
     "level": 1,
     "source": [
      "Importing rating file"
     ]
    },
    {
     "cell_type": "code",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 30,
       "text": [
        "(9L, (10196, 11242, 3L))"
       ],
       "metadata": {}
      }
     ],
     "input": [
      "def parseRelation(line):\n",
      "    fields = line.split(\"\\t\")\n",
      "    \n",
      "    rating = json.loads(fields[3])[\"rating\"]\n",
      "    properties_js = json.loads(fields[4])\n",
      "    user = properties_js[\"subject\"].split(\":\")[1]\n",
      "    movie = properties_js[\"object\"].split(\":\")[1]\n",
      "    \n",
      "    return long(fields[2]) % 10, (int(user), int(movie), long(rating))\n",
      "\n",
      "ratings = sc.textFile('/vagrant/vagrant/relations.dat').filter(lambda line: \"rating.explicit\" in line).map(parseRelation)\n",
      "ratings.first()"
     ],
     "language": "python",
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 31,
       "text": [
        "(9L, (10196, 11242, 3L))"
       ],
       "metadata": {}
      }
     ],
     "input": [
      "def parseEntity(line):\n",
      "    fields = line.split(\"\\t\")\n",
      "    js = json.loads(fields[2])\n",
      "    return int(fields[1]), js[\"title\"]\n",
      "\n",
      "movies = sc.textFile('/vagrant/vagrant/entities.dat').filter(lambda line: \"movie\" in line).map(parseEntity)\n",
      "ratings.first()"
     ],
     "language": "python",
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Got 100000 ratings from 943 users on 1682 movies.\n"
       ]
      }
     ],
     "input": [
      "numRatings = ratings.count()\n",
      "    numUsers = ratings.values().map(lambda r: r[0]).distinct().count()\n",
      "    numMovies = ratings.values().map(lambda r: r[1]).distinct().count()\n",
      "\n",
      "    print \"Got %d ratings from %d users on %d movies.\" % (numRatings, numUsers, numMovies)"
     ],
     "language": "python",
     "prompt_number": 32
    },
    {
     "cell_type": "heading",
     "metadata": {},
     "level": 1,
     "source": [
      "Splitting data for test and validation"
     ]
    },
    {
     "cell_type": "code",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training: 60024, validation: 20435, test: 19541\n"
       ]
      }
     ],
     "input": [
      "numPartitions = 4\n",
      "\n",
      "training = ratings.filter(lambda x: x[0] < 6).values().repartition(numPartitions).cache()\n",
      "validation = ratings.filter(lambda x: x[0] >= 6 and x[0] < 8).values().repartition(numPartitions).cache()\n",
      "test = ratings.filter(lambda x: x[0] >= 8).values().cache()\n",
      "\n",
      "numTraining = training.count()\n",
      "numValidation = validation.count()\n",
      "numTest = test.count()\n",
      "\n",
      "print \"Training: %d, validation: %d, test: %d\" % (numTraining, numValidation, numTest)"
     ],
     "language": "python",
     "prompt_number": 33
    },
    {
     "cell_type": "heading",
     "metadata": {},
     "level": 1,
     "source": [
      "Create training test and validation data"
     ]
    },
    {
     "cell_type": "code",
     "metadata": {},
     "outputs": [
      {
       "ename": "Py4JJavaError",
       "evalue": "An error occurred while calling o721.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/tmp/training already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:132)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1041)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:940)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:849)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1164)\n\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:443)\n\tat org.apache.spark.api.java.JavaRDD.saveAsTextFile(JavaRDD.scala:32)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\n",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-34-9c47187f425f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/tmp/training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/tmp/validation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/tmp/test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/lib/spark/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36msaveAsTextFile\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m   1286\u001b[0m         \u001b[0mkeyed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitionsWithIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m         \u001b[0mkeyed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bypass_serializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1288\u001b[0;31m         \u001b[0mkeyed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m     \u001b[0;31m# Pair functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/lib/spark/python/build/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[0;32m--> 538\u001b[0;31m                 self.target_id, self.name)\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/lib/spark/python/build/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    299\u001b[0m                     \u001b[0;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[1;32m    301\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 raise Py4JError(\n",
        "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o721.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/tmp/training already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:132)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1041)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:940)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:849)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1164)\n\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:443)\n\tat org.apache.spark.api.java.JavaRDD.saveAsTextFile(JavaRDD.scala:32)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\n"
       ]
      }
     ],
     "input": [
      "training.repartition(1).saveAsTextFile(\"/tmp/training\")\n",
      "validation.repartition(1).saveAsTextFile(\"/tmp/validation\")\n",
      "test.repartition(1).saveAsTextFile(\"/tmp/test\")"
     ],
     "language": "python",
     "prompt_number": 34
    }
   ]
  }
 ],
 "cells": [],
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0
}